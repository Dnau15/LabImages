{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1407d943-d2a2-4a7a-895d-dd912039941c",
   "metadata": {},
   "source": [
    "# Natural Language Processing. Lesson 2. Text processing basics\n",
    "\n",
    "In this lab, we will cover a wide range of the Text Processing concepts:\n",
    "- Sentence Segmentation, \n",
    "- Lowercasing,\n",
    "- Stop Words Removal,\n",
    "- Lemmatization,\n",
    "- Stemming,\n",
    "- Byte-Pair Encoding (BPE),\n",
    "- Edit Distance. \n",
    "\n",
    "These methods help to understand how computers can work with human language. In other words, they are essential for unlocking the `meaning` hidden within text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b5da3-110f-4c6e-8764-63cda762c57b",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "\n",
    "Sentence segmentation is a fundamental step that involves dividing a block of text into individual sentences, typically separated by punctuation marks. This method was considered in the previous lesson, so you already should be familiar with it. This technique may be used in:\n",
    "- Part-of-Speech (POS): accurate boundaries between sentences are required for assigning grammatical labels like nouns, verbs, and adjectives.\n",
    "- Sentiment Analysis: understanding the sentiment (positive, negative, neutral) of a sentence also relies on exact boundaries\n",
    "\n",
    "And much more tasks need splitting the text on sentences. It can be performed using already known libraries: nltk or spaCy. Let's use nltk here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b044f",
   "metadata": {},
   "source": [
    "First of all install the required library and import it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce80953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0178922e-cba3-41e4-aa83-acdf7fa38181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample text.', 'It contains multiple sentences.', 'Can we segment it?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anastasia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text = \"This is a sample text. It contains multiple sentences. Can we segment it?\"\n",
    "\n",
    "# tokenize into sentences using nltk.sent_tokenize()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(sentences)\n",
    "# Output: ['This is a sample text.', 'It contains multiple sentences.', 'Can we segment it?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70f81c",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "Complete the following code. Split the text into sentences and save tokens into the `sentences` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4775af19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anastasia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ce789fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There exist some challenges for this technique.', 'The main problem is language specificity because sentence segmentation rules can differ across languages.', 'For example, Japanese omits spaces between words, making segmentation more complex.']\n"
     ]
    }
   ],
   "source": [
    "text = \"There exist some challenges for this technique. The main problem is \\\n",
    "language specificity because sentence segmentation rules can differ across \\\n",
    "languages. For example, Japanese omits spaces between words, making \\\n",
    "segmentation more complex. Punctuation ambiguity also may be the problem because \\\n",
    "of their complexity. Certain punctuation marks like ellipses (...) or colons (:) \\\n",
    "might not always indicate sentence boundaries, requiring context-aware approaches.\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences[:3])\n",
    "\n",
    "# Output: ['There exist some challenges for this technique.', 'The main problem is\n",
    "# language specificity because sentence segmentation rules can differ across languages.',\n",
    "# 'For example, Japanese omits spaces between words, making segmentation more complex.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "470d875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sentences[:3] == [\n",
    "    \"There exist some challenges for this technique.\",\n",
    "    \"The main problem is language specificity because \\\n",
    "sentence segmentation rules can differ across \\\n",
    "languages.\",\n",
    "    \"For example, Japanese omits \\\n",
    "spaces between words, making segmentation \\\n",
    "more complex.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c1aa6-6680-4ad6-aa0d-278c782580c0",
   "metadata": {},
   "source": [
    "## Lowercasing\n",
    "\n",
    "Lowercasing, as the name suggests, is the process of converting all characters in a text string to lowercase. This seemingly simple step plays a crucial role in NLP tasks for several reasons:\n",
    "- Consistency and focus on a word meaning: 'Apple' and 'apple' should be treated identically in terms of their meaning\n",
    "- Improved performance: the same trick with apples reduces the number of unique words representations. Since many NLP algorithms rely on statistical analysis, it allows to avoid overfitting to specific capitalization patterns\n",
    "- Compatibility with NLP tools: many NLP libraries and tools work primarily with lowercase text. Lowercasing ensures compatibility and avoids potential errors or inconsistencies.\n",
    "\n",
    "For applying the Lowercasing we can use a simple Python built-in function for strings: `string.lower()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab620e1-dbf8-4e06-8966-974c54da2a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example text.\n"
     ]
    }
   ],
   "source": [
    "text = \"ThIs Is AN ExaMple Text.\"\n",
    "\n",
    "# apply the .lower() function\n",
    "lowercased_text = text.lower()\n",
    "\n",
    "print(lowercased_text)\n",
    "# Output: this is an example text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb0b08",
   "metadata": {},
   "source": [
    "Except the .lower() the Python has .upper() function for strings. Converting all letters to their capital form is called `Uppercasing` and it also may be applied in NLP (rarely):\n",
    "- Emphasis detection: in some cases, uppercase letters can indicate emphasis in text, like headlines, slogans, or acronyms. Uppercasing can help identify potential emphasis markers\n",
    "- Specific NLP libraries: certain NLP libraries might have functionalities that work better with uppercase text, though this is less common. (Always refer to the documentation for specific tools)\n",
    "- Named entity recognition (NER): in NER tasks, proper nouns (names of people, places, organizations) are often capitalized. Uppercasing text can be a preprocessing step to highlight potential named entities, but additional checks are needed for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15284c2c",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "Fill the gaps in the following code. Convert letters in 2 strings according to the meaning in the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b42bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLY THE UPPERCASING\n",
      "apply the lowercasing\n"
     ]
    }
   ],
   "source": [
    "# no additional libraries are required\n",
    "upper = \"aPplY thE uPpErCASiNg\"\n",
    "lower = \"appLy ThE LowERcAsINg\"\n",
    "\n",
    "# apply the needed functions:\n",
    "upper_result = upper.upper()\n",
    "lower_result = lower.lower()\n",
    "\n",
    "print(upper_result)\n",
    "print(lower_result)\n",
    "\n",
    "# Output:\n",
    "# APPLY THE UPPERCASING\n",
    "# apply the lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a101932",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert upper_result == \"APPLY THE UPPERCASING\"\n",
    "assert lower_result == \"apply the lowercasing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2fb22-edf9-4e9f-9922-e446343763b7",
   "metadata": {},
   "source": [
    "## Stop Words Removal\n",
    "\n",
    "Stop words are frequently occurring words that are often removed during text processing to focus on meaningful words. Examples include articles (\"the\", \"a\", \"an\"), prepositions (\"of\", \"to\", \"in\"), conjunctions (\"and\", \"but\", \"or\"), and pronouns (\"I\", \"you\", \"he\"). While these words are essential for human language construction, they often provide minimal value for NLP tasks, thus there are several reasons to get rid of them:\n",
    "- Focus on the content and improved efficiency: removing stop words allows to keep much meaning and less words amount for optimizing the algorithms\n",
    "- Statistical analysis: stop words can skew the results of statistical analysis in NLP tasks that rely on word frequency. Removing them reduces this bias and promotes to a more accurate representation of the important words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27f6d73-270d-45da-8619-c8edd2b28800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'stop', 'words.']\n"
     ]
    }
   ],
   "source": [
    "# Use an available in nltk method stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download the stop words list\n",
    "# quiet=True hides messages that .download() might display\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "# retrieve the list with stop words in english\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "text = \"This is an example sentence with some stop words.\"\n",
    "\n",
    "# remove all stop words using the loop\n",
    "filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_words)\n",
    "# Output: ['example', 'sentence', 'stop', 'words.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff136b1",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "Fill the gaps in the following cell. Get rid of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "159f7bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download the stop list\n",
    "nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c8b50e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog.']\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# get the stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# remove the meaningless words\n",
    "filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_words)\n",
    "\n",
    "# Output: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91aac5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert filtered_words == [\"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d7974",
   "metadata": {},
   "source": [
    "You should always be careful with this method. Consider whether stop word removal is beneficial for your specific NLP task. Stop words removal might promote to a loss of context (\"I don't like it\" vs. \"I like it\" - the sentiment of the sentences can be lost). Use only domain-specific stop words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d58ac-d436-45ac-953c-83d012cd4c4b",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization involves  reducing words to their base or dictionary form, known as the lemma. This helps to group related words together and improve the accuracy of NLP models.\n",
    "\n",
    "`Lemma` is the canonical form of a word, also referred to as its base or dictionary form (runs - run, keeps - keep, apples - apple).\n",
    "\n",
    "Lemmatization algorithms use a dictionary and morphological analysis and rules to identify the base form of a word.\n",
    "\n",
    "What for?\n",
    "- Improved accuracy: grouping words with the same meaning into their base form helps to handle different variations of the same concept\n",
    "- Reduced vocabulary size and memory usage: lemmatization reduces the number of unique words an NLP model needs to process\n",
    "\n",
    "`WordNetLemmatizer` will help us in this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d69808e-3b41-4082-9407-53360e8f8c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rock', 'corpus', 'cry']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"rocks\", \"corpora\", \"cries\"]\n",
    "# apply the lemmatizer to all words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(lemmatized_words)\n",
    "# Output: ['rock', 'corpus', 'cry']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6457e0",
   "metadata": {},
   "source": [
    "EXERCISE 4\n",
    "\n",
    "Fill the gaps. Convert given words to their original form. The task includes the [Part-of-Speech method](https://www.geeksforgeeks.org/nlp-part-of-speech-default-tagging/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce237e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the WordNetLemmatizer and declare its instance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62180d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('running', 'VBG'), ('dog', 'NNS'), ('carry', 'VBZ'), ('cook', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "words = [\"running\", \"dogs\", \"carries\", \"cooks\"]\n",
    "lemmatized_words = []\n",
    "\n",
    "# assign a corresponding part of the speech to each word\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    # bring each word to its cannonical form\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    lemmatized_words.append((lemmatized_word, tagged_words[i][1]))\n",
    "\n",
    "print(lemmatized_words)\n",
    "# Output: [('running', 'VBG'), ('dog', 'NNS'), ('carry', 'VBZ'), ('cook', 'NNS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f10d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lemmatized_words == [\n",
    "    (\"running\", \"VBG\"),\n",
    "    (\"dog\", \"NNS\"),\n",
    "    (\"carry\", \"VBZ\"),\n",
    "    (\"cook\", \"NNS\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602576b-bc1a-4744-8632-33df9c2a16b4",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming reduces words to their stems or root form, often by removing suffixes, in a more heuristic approach (running - run, jumped - jump, books - book). Similar to the Lemmatization, but what is the difference?\n",
    " - Stemming relies on suffix removal rules which might lead to a wrong word (running - runn), while Lemmatization uses the morphological analysis\n",
    " - Stemming is faster and simpler \n",
    " - Application: Stemming is more preferable when computational efficiency is a priority and general understanding of the core meaning is sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9af22fa2-8780-4ce9-9260-340995893579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'rock', 'beauti']\n"
     ]
    }
   ],
   "source": [
    "# import a simple module for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# and create its instance\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"rocks\", \"beautifully\"]\n",
    "\n",
    "# apply stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)\n",
    "# Output: ['run', 'rock', 'beauti']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d33fb",
   "metadata": {},
   "source": [
    "EXERCISE 5\n",
    "\n",
    "Fill the gaps and observe how the stemming works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2918674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed word is: beauti\n"
     ]
    }
   ],
   "source": [
    "# import the needed module for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# and create its instance\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text = input(\n",
    "    \"If you want to finish the exercise, enter the word 'end'.\\nEnter some text: \"\n",
    ")\n",
    "while text != \"end\":\n",
    "    stemmed = stemmer.stem(text)\n",
    "    print(\"The stemmed word is:\", stemmed)\n",
    "    text = input(\"Enter some text: \")\n",
    "\n",
    "# You may input words running, beautiful, useless, cries, dreamer, end\n",
    "# Output:\n",
    "# The stemmed word is: run\n",
    "# The stemmed word is: beauti\n",
    "# The stemmed word is: useless\n",
    "# The stemmed word is: cri\n",
    "# The stemmed word is: dreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b62c6-1633-4f60-87c2-84797f36aa77",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE is a technique used in Natural Language Processing (NLP) for subword tokenization. Unlike traditional tokenization that splits text into individual words, BPE breaks down text into smaller units considering the vocabulary size and morphology of the language. This approach can be particularly beneficial when dealing with large vocabularies or rare words. The algoritghm:\n",
    "1. Initial vocabulary: BPE starts with the individual characters in the text as the initial vocabulary.\n",
    "2. Merging frequent pairs: it iteratively analyzes the training text and identifies the most frequent pair of characters or subwords (considering existing merged units).\n",
    "3. Replacing pairs: this most frequent pair is replaced with a new symbol not present in the vocabulary. The new symbol represents the merged subword.\n",
    "4. Vocabulary update: the vocabulary is updated to include the newly created symbol.\n",
    "5. Repeat: steps 2-4 are repeated for a predefined number of iterations or until a desired vocabulary size is reached.\n",
    "\n",
    "Applications:\n",
    "- Machine translation: for effective handling vocabulary differences between languages\n",
    "- Text classification and summarization: BPE proves a richer representation of words and captures morphological information\n",
    "- Large Language Models (LLMs): BPE allowes LLMs to handle the vast vocabulary encountered in real-world text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e4135cf-365f-4e28-b903-538669439f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "945b3193-5242-4e8d-a797-53dabcbd6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TemplateProcessing for templates\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# these tokens have specific meanings within the tokenizer's\n",
    "# vocabulary and are not part of the regular text\n",
    "# UNK - unknown words not encountered during training\n",
    "# CLS - indicate the beginning of a sentence\n",
    "# SEP - separates sentences\n",
    "# PAD - for padding sequences to a fixed length\n",
    "# MASK - employed in tasks like masked language modeling,\n",
    "# where certain words are masked and the model predicts them\n",
    "special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "\n",
    "# TemplateProcessing's instances define a template for how the tokenizer should\n",
    "# handle text during the encoding and decoding process\n",
    "temp_proc = TemplateProcessing(\n",
    "    # single - specifies the format for encoding single sentences\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5348f7ef-20f7-4aac-9fb6-f46087066338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.decoders import BPEDecoder\n",
    "\n",
    "# create the instance of the Tokenizer\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.decoder = BPEDecoder()\n",
    "tokenizer.post_processor = temp_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc9e61d-1c9c-4c8d-84ec-149bd123d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e5d4f76-feba-42ec-a8e3-d9f9e0c61f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "nltk.download(\"gutenberg\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=5000, special_tokens=special_tokens)\n",
    "shakespeare = [\" \".join(s) for s in gutenberg.sents(\"shakespeare-macbeth.txt\")]\n",
    "tokenizer.train_from_iterator(shakespeare, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8efe49-2cbe-4bf0-8f91-f6b84f1570f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'b', 'pe', 'is', 'a', 'd', 'at', 'a', 'com', 'pre', 'ss', 'ion', 'te', 'ch', 'ni', 'que', 'use', 'd', 'in', 'n', 'lp', 'for', 'to', 'ken', 'iz', 'ation', '.', '[SEP]']\n",
      "['[CLS]', 'is', 'this', 'a', 'danger', 'which', 'i', 'see', 'before', 'me', ',', 'the', 'handle', 'toward', 'my', 'hand', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.encode(\n",
    "        \"BPE is a data compression technique used in NLP for tokenization.\"\n",
    "    ).tokens\n",
    ")\n",
    "print(\n",
    "    tokenizer.encode(\n",
    "        \"Is this a danger which I see before me, the handle toward my hand?\"\n",
    "    ).tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22796e-92f0-4d6b-85c0-eca37e819ee3",
   "metadata": {},
   "source": [
    "## Levenshtein edit distance\n",
    "\n",
    "Edit distance measures the similarity between two strings by counting the minimum number of operations needed to transform one string into the other.\n",
    "\n",
    "[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance#Example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6969f720-4edf-4dfb-9fb9-8c49561cc901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Levenshtein==0.25.1 (from python-Levenshtein)\n",
      "  Downloading Levenshtein-0.25.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein==0.25.1->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading Levenshtein-0.25.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m671.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.25.1 python-Levenshtein-0.25.1 rapidfuzz-3.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76bc7092-c2be-4324-b674-2ae1e6a91b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance between 'kitten' and 'sitting': 3\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "word1 = \"kitten\"\n",
    "word2 = \"sitting\"\n",
    "distance = Levenshtein.distance(word1, word2)\n",
    "print(f\"Edit distance between '{word1}' and '{word2}': {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a169b-7437-43fe-9bb7-f705037289f4",
   "metadata": {},
   "source": [
    "# Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9fb6a-d744-4950-a9de-9b03febf7e70",
   "metadata": {},
   "source": [
    "[Competition](https://www.kaggle.com/t/6dcb6f9def724f9f82050e9092952dd6)\n",
    "\n",
    "The aim of the competition is to count the 10 most frequent words in the plays presented in the `data.txt` file.\n",
    "\n",
    "In order to count the frequent words correctly, you must perform lemmatization and remove stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa7c6461-6389-4c36-be91-63c493ee1e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-caesar.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data.txt\") as f:\n",
    "    data = f.read()\n",
    "plays = data.split(\"\\n\")\n",
    "plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b669c3d-5f63-460c-af95-ff71bc42f597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt 887071\n",
      "austen-persuasion.txt 466292\n",
      "austen-sense.txt 673022\n",
      "shakespeare-macbeth.txt 100351\n",
      "shakespeare-hamlet.txt 162881\n",
      "shakespeare-caesar.txt 112310\n"
     ]
    }
   ],
   "source": [
    "plays_dict = {}\n",
    "\n",
    "for play in plays:\n",
    "    plays_dict[play] = gutenberg.raw(play)\n",
    "    print(play, len(plays_dict[play]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bababbbd-c25e-4c02-bcfd-3dcb69f107b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_frequent_words(text, topk=10):\n",
    "    # your implementation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f4d7ab-915d-4b6b-a537-6373f47e8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = {}\n",
    "for play, text in plays_dict.items():\n",
    "    top_words[play] = top_frequent_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5e1d600-f6cb-4df8-9522-52810cd349bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"id,count\\n\")\n",
    "    for play, counts in top_words.items():\n",
    "        for i, count in enumerate(counts):\n",
    "            f.write(f\"{play}_{i},{count[1]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
